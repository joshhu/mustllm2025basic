{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNzGlTw4zeWA"
      },
      "source": [
        "# Handling multiple sequences (PyTorch)\n",
        "* 如何處理多個序列？\n",
        "* 如何處理不同長度的多個序列？\n",
        "* 詞彙索引是模型正常運作的唯一輸入嗎？\n",
        "* 是否存在序列太長之類的情況？"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uMNE6Y5G0WCo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqjAX83_zeWE"
      },
      "source": [
        "安裝套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IVmlVSLzeWE"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "將序列轉換為數字list。將此數字list轉換為張量並將其發送到模型"
      ],
      "metadata": {
        "id": "2KioGxLW6JM-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWrs2Nq9zeWF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlarTfhfzeWG",
        "outputId": "a5520206-828b-4961-c99d-78b48907722d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
              "          2607,  2026,  2878,  2166,  1012,   102]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上例向模型發送了單個序列，而Transformers 模型預設需要多個句子。此例分詞器應用到sequence，不僅將輸入 ID 列表轉換為張量，而且還在其上添加了一個維度(加上`[]`)"
      ],
      "metadata": {
        "id": "sViI-JK36egr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "735FdxFUzeWG",
        "outputId": "993c8652-61cb-4378-9b94-364606ee8636",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### padding\n",
        "向模型提供多個句子時，稱為batch(批次)，批次允許模型發揮作用。使用多個序列就像使，但是將兩個（或更多）句子批次在一起時，它們的長度可能不同。模型只吃是矩形形狀的張量，因此您無法將輸入 ID 清單直接轉換為張量。為了解決這個問題，我們通常會填入一個「填充值」稱為padding。"
      ],
      "metadata": {
        "id": "ZlpRobqW64lt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "下面ID無法輸入模型"
      ],
      "metadata": {
        "id": "9STmbNKl7dAa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jwy1AjouzeWG"
      },
      "outputs": [],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "為了解決這個問題，我們將使用padding使張量具有矩形形狀。填充透過在值較少的句子上添加一個稱為「填充標記」的特殊單字來確保所有句子具有相同的長度。如果有 10 個包含 10 個單字的句子和 1 個包含 20 個單字的句子，填充將確保所有句子都有 20 個單字。範例中產生的張量如下所示："
      ],
      "metadata": {
        "id": "0RT03t5Y7gLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UcsJRcxdzeWH"
      },
      "outputs": [],
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### padding token\n",
        "padding的token ID 必須在預訓練時就先定義好，在tokenizer的設定檔中會指定這個token。下例使用並通過模型單獨發送兩個句子並一起批處理"
      ],
      "metadata": {
        "id": "RvxETCy27xjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yGXSVFemzeWH",
        "outputId": "a8750d8d-5c92-4232-b2e6-f2974688f6f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 一模一樣的文字值不同\n",
        "批量預測中的邏輯有問題：第二行應該與第二句話的邏輯相同，但我們得到了完全不同的值，這是因為 Transformer 模型的關鍵特徵是將每個token置於上下文中的注意力層。會考慮到padding token也被拿去做注意力機制，但padding不應該參加注意力機制。為了在通過模型傳遞不同長度的單一句子或傳遞具有相同句子，並應用padding的批次時獲得相同的結果，我們需要告訴這些注意層忽略padding。這是透過使用`attention_mask`來完成的。"
      ],
      "metadata": {
        "id": "6flKkQXv82TQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`attention_mask`是與輸入 ID 張量形狀完全相同的張量，用 0 和 1 填充：1 表示應注意相應的token，0 表示不應注意相應的token（即，應忽略它們）模型的注意力層）。"
      ],
      "metadata": {
        "id": "Z11dDIOI9id4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iyKMCuoZzeWH",
        "outputId": "b54cd35f-536c-4521-aa86-bbbab4f6217e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 更長的句子\n",
        "對於 Transformer 模型，模型的處理序列長度是有限的。大多數模型可處理最多 512 或 1024 個標記的序列，並且當要求處理更長的序列時會當機。這個問題有兩種解決方案：\n",
        "\n",
        "* 使用支援的序列長度較長的模型。\n",
        "* 截斷序列。\n",
        "\n",
        "不同模型具有不同的支援序列長度，有些模型專門處理非常長的序列。Longformer是一個例子，另一個例子是LED。否則，我們建議您透過指定參數來截斷序列`max_sequence_length`："
      ],
      "metadata": {
        "id": "Dm0XmIfm_YCM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWWLMmSxzeWH"
      },
      "outputs": [],
      "source": [
        "sequence = sequence[:max_sequence_length]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}